from typing import Dict, Optional, Tuple, Any, List
import logging
import copy

from overrides import overrides
import torch
from torch.nn.modules import Dropout
import numpy

import gc
from allennlp.common.checks import check_dimensions_match, ConfigurationError
from allennlp.data import Vocabulary
from allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder, Embedding, InputVariationalDropout
from allennlp.modules.seq2seq_encoders import PassThroughEncoder
from allennlp.modules.matrix_attention.bilinear_matrix_attention import BilinearMatrixAttention
from allennlp.modules import FeedForward
from allennlp.models.model import Model
from allennlp.nn import InitializerApplicator, RegularizerApplicator, Activation
from allennlp.nn.util import get_text_field_mask
from allennlp.nn.util import get_lengths_from_binary_sequence_mask
from myallennlp.metric import IterativeLabeledF1Measure
from myallennlp.modules.refiner.srl_score_based_refiner import SRLScoreBasedRefiner

logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
from itertools import chain
from allennlp.models.archival import load_archive
from allennlp.nn.util import masked_softmax, weighted_sum

from myallennlp.dataset_readers.MultiCandidatesSequence import MultiCandidatesSequence
from myallennlp.modules.reparametrization.gumbel_softmax import hard, _sample_gumbel, inplace_masked_gumbel_softmax

from myallennlp.models.srl_graph_base import SRLGraphParserBase

from myallennlp.modules.refiner.refiner import SRLRefiner
from myallennlp.modules.refiner.direct_refiner import DirectSRLRefiner
from myallennlp.modules.sparsemax import Sparsemax
@Model.register("srl_graph_parser_refine")
class SRLGraphParserRefine(Model):
    """
    A Parser for arbitrary graph stuctures.

    Parameters
    ----------
    vocab : ``Vocabulary``, required
        A Vocabulary, required in order to compute sizes for input/output projections.
    text_field_embedder : ``TextFieldEmbedder``, required
        Used to embed the ``tokens`` ``TextField`` we get as input to the model.
    encoder : ``Seq2SeqEncoder``
        The encoder (with its own internal stacking) that we will use to generate representations
        of tokens.
    tag_representation_dim : ``int``, required.
        The dimension of the MLPs used for arc tag prediction.
    arc_representation_dim : ``int``, required.
        The dimension of the MLPs used for arc prediction.
    tag_feedforward : ``FeedForward``, optional, (default = None).
        The feedforward network used to produce tag representations.
        By default, a 1 layer feedforward network with an elu activation is used.
    arc_feedforward : ``FeedForward``, optional, (default = None).
        The feedforward network used to produce arc representations.
        By default, a 1 layer feedforward network with an elu activation is used.
    pos_tag_embedding : ``Embedding``, optional.
        Used to embed the ``pos_tags`` ``SequenceLabelField`` we get as input to the model.
    dropout : ``float``, optional, (default = 0.0)
        The variational dropout applied to the output of the encoder and MLP layers.
    input_dropout : ``float``, optional, (default = 0.0)
        The dropout applied to the embedded text input.
    edge_prediction_threshold : ``int``, optional (default = 0.5)
        The probability at which to consider a scored edge to be 'present'
        in the decoded graph. Must be between 0 and 1.
    initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)
        Used to initialize the model parameters.
    regularizer : ``RegularizerApplicator``, optional (default=``None``)
        If provided, will be used to calculate the regularization penalty during training.
    """

    def __init__(self,
                 vocab: Vocabulary,
                 base_model_archive: str,
                 refiner: Seq2SeqEncoder,
                 rep_dim:int,
                 encoder: Seq2SeqEncoder = None,
                 train_score: float = 10.0,
                 dropout: float = 0.3,
                 delta_type: str = "hinge_ce",
                 initializer: InitializerApplicator = InitializerApplicator(),
                 regularizer: Optional[RegularizerApplicator] = None) -> None:
        super(SRLGraphParserRefine, self).__init__(vocab, regularizer)
        self.train_score = train_score
        self.delta_type = delta_type
        base_model: SRLGraphParserBase = load_archive(base_model_archive).model
        base_model.gumbel_t = refiner.gumbel_t
        base_model.subtract_gold = refiner.subtract_gold
        base_model.as_base = True
        self.encoder = None if encoder is None else encoder
        assert self.encoder is not None, "have not implemented reuse for now"

        num_labels = self.vocab.get_vocab_size("tags")

        sense_dim = base_model._pred_embedding.get_output_dim()

        encoder_dim = self.encoder.get_output_dim() if self.encoder else base_model.encoder.get_output_dim()

        self.rep_dim = rep_dim
        self.predicte_rep_feedforward = FeedForward(encoder_dim, 1,
                                                rep_dim,
                                                Activation.by_name("elu")())
        self.argument_rep_feedforward = FeedForward(encoder_dim, 1,
                                                    rep_dim,
                                                Activation.by_name("elu")())
        self.refiner = refiner
        self.refiner.initialize_network(n_tags=num_labels, sense_dim=sense_dim, rep_dim=rep_dim)
        self._dropout = InputVariationalDropout(dropout)

        #   check_dimensions_match(representation_dim, encoder.get_input_dim(), "text field embedding dim", "encoder input dim")

        self._labelled_f1 = IterativeLabeledF1Measure(negative_label=0, negative_pred=0,
                                                      selected_metrics=["F", "l_F", "p_F", "h_S"])
        self._tag_loss = torch.nn.NLLLoss(reduction="none")  # ,ignore_index=-1
        self._sense_loss = torch.nn.NLLLoss(reduction="none")  # ,ignore_index=-1
        self.sparse_max = Sparsemax()
        initializer(self)
        self._pred_embedding = copy.deepcopy(base_model._pred_embedding)  #get a trainable copy of predicate sense embedding in any case
        for param in base_model.parameters():
            param.requires_grad = False
        self.base_model = base_model


    @overrides
    def forward(self,  # type: ignore
                tokens: Dict[str, torch.LongTensor],
                pos_tags: torch.LongTensor,
                dep_tags: torch.LongTensor,
                predicate_candidates: torch.LongTensor = None,
                epoch: int = None,
                predicate_indexes: torch.LongTensor = None,
                sense_indexes: torch.LongTensor = None,
                predicates: torch.LongTensor = None,
                metadata: List[Dict[str, Any]] = None,
                arc_tags: torch.LongTensor = None) -> Dict[str, torch.Tensor]:
        # pylint: disable=arguments-differ
        """
        Parameters
        ----------
        tokens : Dict[str, torch.LongTensor], required
            The output of ``TextField.as_array()``.
        verb_indicator: torch.LongTensor, required.
            An integer ``SequenceFeatureField`` representation of the position of the verb
            in the sentence. This should have shape (batch_size, num_tokens) and importantly, can be
            all zeros, in the case that the sentence has no verbal predicate.
        pos_tags : ``torch.LongTensor``, optional, (default = None).
            The output of a ``SequenceLabelField`` containing POS tags.
        arc_tags : torch.LongTensor, optional (default = None)
            A torch tensor representing the sequence of integer indices denoting the parent of every
            word in the dependency parse. Has shape ``(batch_size, sequence_length, sequence_length)``.
        pred_candidates : torch.LongTensor, optional (default = None)
            A torch tensor representing the sequence of integer indices denoting the parent of every
            word in the dependency parse. Has shape ``(batch_size, predicates_len, batch_max_senses)``.

        predicate_indexes:  shape (batch_size, predicates_len)

        Returns
        -------
        An output dictionary.
        """
        #  torch.cuda.empty_cache()


        # shape (batch_size, predicates_len, batch_max_senses , pred_dim)
        input_dict = self.base_model(tokens,
                                        pos_tags,
                                        dep_tags,
                                        predicate_candidates,
                                        epoch,
                                        predicate_indexes,
                                        sense_indexes,
                                        predicates,
                                        metadata,
                                        arc_tags)

        if arc_tags is not None:
            arc_tags = arc_tags.long()

        output_dict = {
            "tokens": [meta["tokens"] for meta in metadata],
            "arc_tag_probs":  input_dict["arc_tag_probs"],
            "sense_probs":  input_dict["sense_probs"],
        }
        #recompute as needed for refinement
        embedded_candidate_preds = self._pred_embedding(predicate_candidates)

        input_sense_logits  = input_dict["sense_logits"].detach()
        input_arc_tag_logits = input_dict["arc_tag_logits"].detach()
        embedded_text_input = input_dict["embedded_text_input"].detach()

   #     print ("fresh predicate_representation",predicate_representation.size())
        # shape (batch_size, predicates_len, batch_max_senses )
        sense_mask = (predicate_candidates > 0).float()


        predicate_indexes = predicate_indexes.long()

        mask = get_text_field_mask(tokens)

        batch_size, sequence_length = mask.size()

        float_mask = mask.float()

        predicate_mask = (predicate_indexes > -1).float()
        graph_mask = predicate_mask.unsqueeze(-1).unsqueeze(1) * float_mask.unsqueeze(-1).unsqueeze(2)




        if isinstance(self.encoder, FeedForward):
            encoded_text = self._dropout(self.encoder(embedded_text_input))
        else:
            encoded_text = self._dropout(self.encoder(embedded_text_input, mask))

        padding_for_predicate = torch.zeros(size=[batch_size, 1, encoded_text.size(-1)], device=encoded_text.device)

        # shape (batch_size, predicates_len, hidden_dim)
        encoded_text_for_predicate = torch.cat([padding_for_predicate, encoded_text], dim=1)

        #    print ("paded encoded_text_for_predicate",encoded_text_for_predicate.size())
        #    print("encoded_text_for_predicate", encoded_text_for_predicate.size())

        #      print("predicate_indexes", predicate_indexes.size())
        index_size = list(predicate_indexes.size()) + [encoded_text.size(-1)]

        #     print("index_size", index_size)
        #      print("predicate_indexes", predicate_indexes.size())
        effective_predicate_indexes = (predicate_indexes.unsqueeze(-1) + 1).expand(index_size)
        encoded_text_for_predicate = encoded_text_for_predicate.gather(dim=1, index=effective_predicate_indexes)



        if arc_tags is not None:
            soft_tags = torch.zeros(size=input_arc_tag_logits.size(), device=input_arc_tag_logits.device)
            soft_tags.scatter_(3, arc_tags.unsqueeze(3) + 1, 1) * graph_mask

            #    print ("sense_logits",sense_logits.size(),sense_logits)
            #    print ("sense_indexes",sense_indexes.size(),sense_indexes)
            soft_index = torch.zeros(size=input_sense_logits.size(), device=input_sense_logits.device)
            soft_index.scatter_(2, sense_indexes.unsqueeze(2), 1) * sense_mask
        else:
            soft_tags, soft_index = None,None

        if arc_tags is not None:
            output_dict["loss"] = 0

        predicate_hidden = self._dropout(self.predicte_rep_feedforward(encoded_text_for_predicate))
        argument_hidden = self._dropout(self.argument_rep_feedforward(encoded_text))

      #  print ("before feed predicate_representation",predicate_representation.size())
        lists, c_lists, gold_results = self.refiner(predicate_hidden,
                                                    argument_hidden,
                                                    embedded_candidate_preds,
                                                    graph_mask,
                                                    sense_mask,
                                                    predicate_mask,
                                                    input_arc_tag_logits,
                                                    input_sense_logits,
                                                    soft_tags,
                                                    soft_index,
                                                    not isinstance(self.refiner,DirectSRLRefiner))

        if arc_tags is not None and not isinstance(self.refiner,DirectSRLRefiner):
            gold_score_nodes, gold_score_edges, gold_sense_logits, gold_sense_probs, gold_arc_tag_logits, gold_arc_tag_probs = gold_results
            output_dict["arc_tag_probs_g"] = gold_arc_tag_probs
            output_dict["sense_probs_g"] = gold_sense_probs
            gold_scores = gold_score_nodes.unsqueeze(1) + gold_score_edges
            self._labelled_f1(gold_arc_tag_probs, arc_tags + 1, graph_mask.squeeze(-1), gold_sense_probs,
                              predicate_candidates,
                              predicates, gold_scores, soft_tags * input_arc_tag_logits, n_iteration=0)
        else:
            gold_score_nodes, gold_score_edges = None,None
        for i, (arc_tag_logits,arc_tag_probs, sense_probs, sense_logits,  score_nodes, score_edges) \
                in enumerate(zip(*c_lists)):
            if arc_tags is not None:

                loss = self._max_margin_loss(arc_tag_logits,
                                             arc_tag_probs,
                                             arc_tags,
                                             soft_tags,
                                             sense_logits,
                                             sense_probs,
                                             sense_indexes,
                                             soft_index,
                                             score_nodes,
                                             score_edges,
                                             gold_score_nodes,
                                             gold_score_edges,
                                             graph_mask,
                                             sense_mask,
                                             predicate_mask)

                output_dict["loss"] = output_dict["loss"] + loss

                scores = score_nodes.unsqueeze(1).cpu() + score_edges.cpu() if score_nodes is not None else None
                self._labelled_f1(arc_tag_probs, arc_tags + 1, graph_mask.squeeze(-1), sense_probs,
                                  predicate_candidates,
                                  predicates, scores, arc_tag_probs.cpu() * input_arc_tag_logits.cpu(), n_iteration=-i - 1)

        for i, (arc_tag_logits,arc_tag_probs, sense_probs, sense_logits,  score_nodes, score_edges) \
                in enumerate(zip(*lists)):
            output_dict["arc_tag_probs" + str(i)] = arc_tag_probs
            output_dict["sense_probs" + str(i)] = sense_probs

            if arc_tags is not None:

                loss = self._max_margin_loss(arc_tag_logits,
                                             arc_tag_probs,
                                             arc_tags,
                                             soft_tags,
                                             sense_logits,
                                             sense_probs,
                                             sense_indexes,
                                             soft_index,
                                             score_nodes,
                                             score_edges,
                                             gold_score_nodes,
                                             gold_score_edges,
                                             graph_mask,
                                             sense_mask,
                                             predicate_mask)

                output_dict["loss"] = output_dict["loss"] + loss
                # We stack scores here because the f1 measure expects a
                # distribution, rather than a single value.
                #     arc_tag_probs = torch.cat([one_minus_arc_probs, arc_tag_probs*arc_probs], dim=-1)

                scores = score_nodes.unsqueeze(1).cpu() + score_edges.cpu() if score_nodes is not None else None
                self._labelled_f1(arc_tag_probs, arc_tags + 1, graph_mask.squeeze(-1), sense_probs,
                                  predicate_candidates,
                                  predicates, scores, arc_tag_probs.cpu() * input_arc_tag_logits.cpu(), n_iteration=i + 1)

        return output_dict

    @overrides
    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:

        for i in range(10):
            if "arc_tag_probs" + str(i) in output_dict:
                output_dict["predicted_arc_tags" + str(i)] = output_dict["arc_tag_probs" + str(i)].argmax(-1) - 1

                output_dict["sense_argmax" + str(i)] = output_dict["sense_probs" + str(i)].argmax(-1)
        if "arc_tag_probs_g" in output_dict:
            output_dict["predicted_arc_tags_g"] = output_dict["arc_tag_probs_g"].argmax(-1) - 1

            output_dict["sense_argmax_g"] = output_dict["sense_probs_g"].argmax(-1)

        if "arc_tag_probs" in output_dict:
            output_dict["predicted_arc_tags"] = output_dict["arc_tag_probs"].argmax(-1) - 1

            output_dict["sense_argmax"] = output_dict["sense_probs"].argmax(-1)

        return output_dict

    def _max_margin_loss(self, arc_tag_logits,
                         arc_tag_probs,
                         arc_tags,
                         soft_tags,
                         sense_logits,
                         sense_probs,
                         sense_indexes,
                         soft_index,
                         score_nodes,
                         score_edges,
                         gold_score_nodes,
                         gold_score_edges,
                         graph_mask,
                         sense_mask,
                         predicate_mask):
        '''pred_probs: (batch_size, sequence_length, max_senses)'''


        valid_positions = graph_mask.sum().float()

        # shape (batch ,sequence_length,sequence_length ,1)
        if self.delta_type == "l2":
            d1 = arc_tag_probs-soft_tags
            delta_tag = 0.0005 * (d1*d1) * graph_mask
            d2 = sense_probs-soft_index
            delta_sense = 0.0005 *  (d2*d2)  * sense_mask
        else:
            delta_tag = self._tag_loss(torch.nn.functional.log_softmax(arc_tag_logits, dim=-1).permute(0, 3, 1, 2), arc_tags + 1).unsqueeze(-1) * graph_mask
            delta_sense = self._sense_loss(torch.nn.functional.log_softmax(sense_logits, dim=-1).permute(0, 2, 1),sense_indexes).unsqueeze(-1) * sense_mask
        if gold_score_nodes is not None and score_nodes is not None and self.train_score:

            if self.delta_type == "rec" or self.delta_type == "l2":
                node_score_nll =  torch.clamp(((-gold_score_nodes + score_nodes) + delta_sense.sum(-1, keepdim=True) ) * predicate_mask.unsqueeze(-1),
                                             min=0).sum() / valid_positions
                edge_node_score_nll = torch.clamp( ((-gold_score_edges + score_edges) +  delta_tag.sum(-1, keepdim=True) ) * graph_mask,
                    min=0).sum() / valid_positions
            elif  self.delta_type == "no_margin":
                node_score_nll =  torch.clamp(((-gold_score_nodes + score_nodes) ) * predicate_mask.unsqueeze(-1),
                                             min=-10).sum() / valid_positions
                edge_node_score_nll = torch.clamp(
                    ((-gold_score_edges + score_edges) ) * graph_mask,
                    min=-10).sum() / valid_positions
            else:
                node_score_nll =  torch.clamp(((-gold_score_nodes + score_nodes) + .01) * predicate_mask.unsqueeze(-1),
                                             min=0).sum() / valid_positions
                edge_node_score_nll = torch.clamp(
                    ((-gold_score_edges + score_edges) + .01) * graph_mask,
                    min=0).sum() / valid_positions
        else:
            node_score_nll , edge_node_score_nll = 0,0

        score_nll = self.train_score * (node_score_nll + edge_node_score_nll)
        if self.delta_type == "kl_only":

            return (delta_tag.sum() + delta_sense.sum()) / valid_positions  + score_nll # + arc_nll
        elif self.delta_type == "rec"  or self.delta_type == "l2":

            tag_nll = torch.clamp((((-soft_tags + arc_tag_probs) * arc_tag_logits) + delta_tag) * graph_mask,
                                  min=0).sum() / valid_positions

            sense_nll = torch.clamp((((-soft_index + sense_probs) * sense_logits) + delta_sense) * sense_mask,
                                    min=0).sum() / valid_positions

            return sense_nll + tag_nll + score_nll
        elif self.delta_type == "hinge":

            tag_nll = torch.clamp(((-soft_tags + arc_tag_probs) * arc_tag_logits + 1) * graph_mask,
                                  min=0).sum() / valid_positions

            sense_nll = torch.clamp(((-soft_index + sense_probs) * sense_logits + 1) * sense_mask,
                                    min=0).sum() / valid_positions


            return sense_nll + tag_nll + score_nll
        elif  self.delta_type == "no_margin":
            tag_nll = ((torch.clamp((-soft_tags + arc_tag_probs) * arc_tag_logits ,
                                    min=-10) + delta_tag) * graph_mask).sum() / valid_positions

            sense_nll = ((torch.clamp((-soft_index + sense_probs) * sense_logits ,
                                      min=-10) + delta_sense) * sense_mask).sum() / valid_positions

            return sense_nll + tag_nll + score_nll
        elif self.delta_type == "hinge_ce":

            tag_nll = ((torch.clamp((-soft_tags + arc_tag_probs) * arc_tag_logits + 1,
                                    min=0) + delta_tag) * graph_mask).sum() / valid_positions

            sense_nll = ((torch.clamp((-soft_index + sense_probs) * sense_logits + 1,
                                      min=0) + delta_sense) * sense_mask).sum() / valid_positions

            return sense_nll + tag_nll + score_nll
        else:
            assert False



    @overrides
    def get_metrics(self, reset: bool = False) -> Dict[str, float]:

        return self._labelled_f1.get_metric(reset, training=self.training)
